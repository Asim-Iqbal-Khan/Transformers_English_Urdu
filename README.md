# English to Urdu Neural Machine Translation (Fine-Tuning)

This project demonstrates how to fine-tune a pre-trained Transformer model for English-to-Urdu translation using the Hugging Face `transformers` library. The project utilizes the **Helsinki-NLP MarianMT** model and fine-tunes it on a parallel corpus (specifically the UMC005 corpus containing Quranic/religious texts).

## ğŸš€ Project Overview

* **Task:** Machine Translation (English $\to$ Urdu)
* **Base Model:** `Helsinki-NLP/opus-mt-en-ur`
* **Library:** Hugging Face Transformers, Datasets, Evaluate
* **Metric:** BLEU Score (SacreBLEU)
* **Final Result:** BLEU Score of **30.03**

## ğŸ“‚ Dataset

The project uses a custom dataset (uploaded as `umc005-corpus.zip`).
* **Source:** UMC005 Corpus (Quran & Bible parallel texts).
* **Structure:**
    * Training Set: 6,000 sentence pairs.
    * Test Set: 200 sentence pairs.
* **Preprocessing:** The data is tokenized using the MarianTokenizer with a maximum sequence length of 128.

## ğŸ› ï¸ Installation & Requirements

To run this notebook or script, you will need the following dependencies:

```bash
pip install transformers[sentencepiece] datasets evaluate sacrebleu sacremoses
```
## âš™ï¸ Training Configuration

The model was fine-tuned using the `Seq2SeqTrainer` with the following hyperparameters:

| Parameter | Value |
| :--- | :--- |
| **Epochs** | 3 |
| **Learning Rate** | 2e-5 |
| **Batch Size** | 16 (Train & Eval) |
| **Weight Decay** | 0.01 |
| **FP16** | True (GPU Acceleration) |
| **Evaluation Strategy** | Per Epoch |

## ğŸ“Š Results

After training for 3 epochs, the model achieved the following performance on the test set:

* **Validation Loss:** ~1.145
* **BLEU Score:** **30.03**

### Sample Translations (Qualitative Analysis)

Below are examples of translations generated by the fine-tuned model compared to the ground truth (Gold):

**Example 1:**
> **Input:** And does not promote the cause of feeding the poor.
> **Model Output:** Ø§ÙˆØ± Ù…Ø­ØªØ§Ø¬ Ú©Ùˆ Ú©Ú¾Ø§Ù†Ø§ Ú©Ú¾Ù„Ø§Ù†Û’ Ú©ÛŒ ØªØ±ØºÛŒØ¨ Ù†ÛÛŒÚº Ø¯ÛŒØªØ§ Û”

**Example 2:**
> **Input:** By the sun and by its brightness.
> **Model Output:** Ø³ÙˆØ±Ø¬ Ú©ÛŒ Ù‚Ø³Ù… Ø§ÙˆØ± Ø§Ø³ Ú©ÛŒ Ø±ÙˆØ´Ù†ÛŒ Ú©ÛŒ

**Example 3 (Custom Input):**
> **Input:** In the name of God, the Most Gracious, the Most Merciful.
> **Model Output:** Ø´Ø±ÙˆØ¹ Ú©Ø±ØªØ§ ÛÙˆÚº Ø§Ù„Ù„Û ØªØ¹Ø§Ù„ÛŒÙ° Ú©Û’ Ù†Ø§Ù… Ø³Û’ Ø¬Ùˆ Ø¨Ú‘Ø§ Ù…ÛØ±Ø¨Ø§Ù† Ù†ÛØ§ÛŒØª Ø±Ø­Ù… ÙˆØ§Ù„Ø§ ÛÛ’

## ğŸ’» Usage

### Inference with the Fine-Tuned Model

Once the model is trained and saved to `./final_model`, you can use it to translate new sentences using the `pipeline` API:

```python
from transformers import pipeline

# Load the trained model
model_path = "./final_model"
translator = pipeline("translation", model=model_path, tokenizer=model_path)

# Translate text
text = "In the name of God, the Most Gracious, the Most Merciful."
translation = translator(text)

print(translation[0]['translation_text'])
# Output: Ø´Ø±ÙˆØ¹ Ú©Ø±ØªØ§ ÛÙˆÚº Ø§Ù„Ù„Û ØªØ¹Ø§Ù„ÛŒÙ° Ú©Û’ Ù†Ø§Ù… Ø³Û’ Ø¬Ùˆ Ø¨Ú‘Ø§ Ù…ÛØ±Ø¨Ø§Ù† Ù†ÛØ§ÛŒØª Ø±Ø­Ù… ÙˆØ§Ù„Ø§ ÛÛ’
```
## ğŸ“ Project Structure

* **Data Preparation:** Unzipping corpus, parsing `.en` and `.ur` files into a Pandas DataFrame, and converting to Hugging Face Dataset objects.
* **Tokenization:** Utilizing `AutoTokenizer` to prepare inputs and labels.
* **Training:** Fine-tuning utilizing `Seq2SeqTrainer` with `DataCollatorForSeq2Seq` for dynamic padding.
* **Evaluation:** Computing BLEU scores using `sacrebleu`.
* **Inference:** Testing the model on unseen data.

## ğŸ¤ Credits

* **Base Model:** [Helsinki-NLP/opus-mt-en-ur](https://huggingface.co/Helsinki-NLP/opus-mt-en-ur)
* **Library:** Hugging Face Transformers
